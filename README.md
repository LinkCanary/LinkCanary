<p align="center">
  <img src="https://img.shields.io/badge/python-3.10%2B-blue" alt="Python 3.10+">
  <img src="https://img.shields.io/badge/version-0.3-green" alt="Version 0.3">
  <img src="https://img.shields.io/badge/license-MIT-orange" alt="MIT License">
</p>

# LinkCanary

**Find broken links and redirect chains before your visitors do.**

LinkCanary is an open-source site auditing tool built out of a messy website migration. It crawls your site via sitemap, checks every link on every page, and ‚Äî crucially ‚Äî maps each broken link or redirect back to **every page where it appears**. So you don't just get a list of problems, you get an actionable fix list.

Use it from the command line for quick audits, or spin up the included web UI for a dashboard experience with real-time progress and a reports library.

---

## The Story

LinkCanary was born out of a real disaster. After migrating a live business website from Squarespace to Ghost.io, the result was a mess ‚Äî hundreds of 404 errors, broken internal links, and redirect chains that looped back on themselves. The site looked fine on the surface, but underneath, visitors and search engines were hitting dead ends everywhere.

Existing tools would tell you *that* a link was broken, but not *where it lived on your site* ‚Äî which pages actually contained the bad link. When you're staring at 200+ broken URLs, knowing the destination is broken isn't enough. You need to know every page that links to it so you can actually fix the problem.

So LinkCanary was built to solve that specific pain: crawl the entire site, check every link, and for each broken or redirecting URL, show you **exactly which pages it appears on** and how many times. That's the difference between a list of problems and an actionable fix list.

## What It Does

LinkCanary crawls your website via sitemap, checks every link on every page, and generates a report that maps each issue back to its source pages. Here's what it catches:

- **Broken links (4xx, 5xx)** ‚Äî dead ends for users and search engines, mapped to every page where they appear
- **Redirect chains** ‚Äî traces the full hop path (e.g., `301:url1 ‚Üí 302:url2 ‚Üí 200:url3`) so you can update links to point directly to the final destination
- **Redirect loops** ‚Äî catches infinite redirect cycles before your visitors hit them
- **Canonical URL mismatches** ‚Äî identifies pages where the canonical tag doesn't match the served URL
- **Priority classification** ‚Äî issues ranked as critical, high, medium, or low so you fix what matters first
- **Actionable fix recommendations** ‚Äî each issue includes a suggested resolution
- **Occurrence tracking** ‚Äî shows how many pages contain each bad link, so you can prioritize the most widespread problems
- **CSV + interactive HTML reports** ‚Äî share with your team or clients, or plug into your workflow
- **Web-based UI** ‚Äî run audits without touching the terminal

---

## Quick Start

```bash
# Clone and install
git clone https://github.com/LinkCanary/LinkCanary.git
cd LinkCanary
python3 -m venv venv
source venv/bin/activate   # Windows: venv\Scriptsctivate
pip install -e .

# Run your first audit
linkcheck https://yoursite.com/sitemap.xml --skip-ok --html-report report.html --open
```

That's it. LinkCanary will crawl every page in your sitemap, check every link on every page, and open an interactive HTML report in your browser when it's done.

---

## Comparison: LinkCanary vs Screaming Frog vs Lychee

**TL;DR:** Screaming Frog is a desktop SEO crawler. Lychee is a fast Rust link checker. LinkCanary is **automation infrastructure**‚Äîbuilt for CI/CD, webhooks, and occurrence tracking that tells you *where* to fix, not just *what* is broken.

| Feature | LinkCanary | Screaming Frog | Lychee |
|---------|------------|----------------|--------|
| **Runtime** | CLI + Docker + GitHub Actions | Desktop GUI only | CLI only |
| **CI/CD Integration** | ‚úÖ Native GitHub Action, fails builds | ‚ùå Requires manual export/import | ‚ö†Ô∏è Basic exit codes, no GitHub Action |
| **Free Tier Limits** | Unlimited URLs | 500 URLs max | Unlimited URLs |
| **Occurrence Tracking** | ‚úÖ "Broken link X appears on 12 pages" | ‚ùå Lists URLs only | ‚ùå Lists URLs only |
| **Priority Classification** | ‚úÖ Critical/High/Medium/Low | ‚ö†Ô∏è Basic issue types | ‚ùå No priority levels |
| **Webhook Alerts** | ‚úÖ Slack, Discord, Jira, Asana | ‚ùå None | ‚ùå None |
| **Export Formats** | CSV, JSON, Excel, PDF, MDX, Google Sheets | CSV, XLSX, PDF | JSON, HTML |
| **Staging Site Auth** | ‚úÖ Basic Auth, Bearer tokens, Cookies | ‚ö†Ô∏è Manual config per crawl | ‚ùå Limited auth support |
| **Single URL Mode** | ‚úÖ Fast PR checks (`--url`) | ‚ùå Full crawl only | ‚úÖ Yes |
| **Retry Logic** | ‚úÖ Exponential backoff (502/503/504) | ‚ùå Single attempt | ‚ö†Ô∏è Basic retries |
| **robots.txt Compliance** | ‚úÖ Respects by default | ‚úÖ Yes | ‚ùå Ignores |
| **JavaScript Rendering** | ‚ö†Ô∏è Planned | ‚úÖ Yes | ‚ùå No |
| **Crawl Speed** | Fast (Python/async) | Medium (desktop limited) | **Fastest** (Rust) |
| **Pricing** | Free (Open Source) | ¬£149/year | Free |

### When to use which:

**Choose Screaming Frog if...**
- You're an SEO analyst doing deep technical audits (hreflang, canonical analysis, log file analysis)
- You need JavaScript rendering for SPAs
- You prefer GUI visualization over automation
- **Limitation:** Can't run in CI/CD, can't trigger webhooks, manual exports only

**Choose Lychee if...**
- You need a *fast* link checker for huge sites (100k+ URLs)
- You want a simple "pass/fail" binary result
- You don't need to know *which pages* contain the broken link
- **Limitation:** No occurrence data, no workflow integrations, basic reporting

**Choose LinkCanary if...**
- You want **CI/CD integration** (fail builds on broken links before deployment)
- You need to **track where broken links appear** (occurrence count) for efficient fixing
- You want **webhook alerts** (Slack notification when production site breaks)
- You manage **staging environments** with authentication (basic auth, bearer tokens)
- You export reports for **content teams** (Excel, PDF, MDX for Ghost CMS)
- You want **smart retry logic** (distinguishes real 404s from temporary 503s)

### The Automation Gap

Screaming Frog is an *audit tool*. Lychee is a *checker*. LinkCanary is **infrastructure** that prevents broken links from reaching production and notifies you when external sites break yours.

| Workflow | Screaming Frog | Lychee | LinkCanary |
|----------|---------------|--------|------------|
| **PR Review** | Export CSV ‚Üí Email ‚Üí Fix later | Terminal output only | üü¢ **Auto-fail build, comment PR with occurrence data** |
| **Production Monitoring** | Manual monthly crawl | Not designed for it | üü¢ **Scheduled checks + Slack alerts** |
| **Team Assignment** | Export to Sheets, assign manually | N/A | üü¢ **Auto-create Jira/Asana tickets** |
| **Staging Checks** | Requires desktop + manual auth config | Limited auth | üü¢ **Built-in auth headers, runs in CI** |

**Pro tip:** Many teams use **Lychee for speed** in large monorepos and **LinkCanary for intelligence** in critical content workflows. They're complementary, not competitors.

---

## Usage

### Command Line

```bash
# Basic audit ‚Äî outputs link_report.csv
linkcheck https://example.com/sitemap.xml

# Internal links only, skip healthy links
linkcheck https://example.com/sitemap.xml --internal-only --skip-ok

# Slower crawl rate to be polite to the server
linkcheck https://example.com/sitemap.xml -o report.csv --delay 2.0

# Quick test on first 10 pages with verbose output
linkcheck https://example.com/sitemap.xml --max-pages 10 --verbose

# Treat subdomains (blog.example.com) as internal
linkcheck https://example.com/sitemap.xml --include-subdomains

# Only check pages modified since a specific date
linkcheck https://example.com/sitemap.xml --since 2025-01-01

# Full audit with HTML report
linkcheck https://example.com/sitemap.xml --html-report report.html --open
```

### Generate HTML Report from Existing CSV

Already have a CSV from a previous run? Convert it to an interactive HTML report:

```bash
linkcheck-report link_report.csv -o report.html --open
```

### Web UI

For teams or non-technical users, LinkCanary includes a web-based dashboard.

```bash
cd linkcanary-ui
pip install -e .
linkcanary-ui --open
```

Open [http://localhost:3000](http://localhost:3000) in your browser.

For production workloads, enable Celery/Redis for background task management:

```bash
pip install -e ".[celery]"
export LINKCANARY_USE_CELERY=true
redis-server &
celery -A backend.tasks.celery_app worker &
linkcanary-ui
```

**Web UI features:**

- Dashboard with one-click crawl launch
- Advanced configuration options
- Real-time progress monitoring
- Reports library with search and filtering
- Interactive report viewer
- CSV and HTML report downloads

---

## CLI Options

| Flag | Default | Description |
|------|---------|-------------|
| `-o, --output` | `link_report.csv` | Output file path |
| `-d, --delay` | `0.5` | Seconds between requests |
| `-t, --timeout` | `10` | Request timeout in seconds |
| `--internal-only` | `false` | Only check internal links |
| `--external-only` | `false` | Only check external links |
| `--skip-ok` | `false` | Exclude 200 OK links from report |
| `--max-pages` | none | Limit pages to crawl (useful for testing) |
| `-v, --verbose` | `false` | Show detailed progress |
| `--user-agent` | `LinkCanary/1.0` | Custom User-Agent string |
| `--expand-duplicates` | `false` | Show all occurrences instead of aggregating |
| `--include-subdomains` | `false` | Treat subdomains as internal links |
| `--since` | none | Only crawl pages modified after date (YYYY-MM-DD) |
| `--html-report` | none | Generate HTML report at specified path |
| `--open` | `false` | Open HTML report in browser after generation |

---

## Understanding the Report

LinkCanary outputs a CSV (and optionally an interactive HTML report) with the following columns:

| Column | Description |
|--------|-------------|
| `source_page` | The page where the problematic link was found |
| `occurrence_count` | Number of pages containing this link |
| `link_url` | The broken or redirecting URL |
| `status_code` | HTTP response code |
| `issue_type` | `broken` ¬∑ `redirect` ¬∑ `redirect_chain` ¬∑ `canonical_redirect` ¬∑ `redirect_loop` ¬∑ `ok` |
| `priority` | `critical` ¬∑ `high` ¬∑ `medium` ¬∑ `low` |
| `redirect_chain` | Full redirect path with status codes (e.g., `301:url1 ‚Üí 302:url2 ‚Üí 200:url3`) |
| `final_url` | Where the link ultimately resolves |
| `recommended_fix` | Suggested action to resolve the issue |

### Exit Codes

| Code | Meaning |
|------|---------|
| `0` | No issues found |
| `1` | Broken links or redirects detected |
| `2` | Crawl failure (couldn't fetch sitemap or fatal error) |

Exit codes make it easy to integrate LinkCanary into CI/CD pipelines ‚Äî fail the build if broken links are introduced.

---

## Use Cases

- **Site migrations** ‚Äî moved from Squarespace, WordPress, or another CMS? Verify that old URLs resolve correctly and catch the 404s and redirect loops that migrations inevitably create
- **Content teams** ‚Äî audit your blog or docs site after restructuring URLs or reorganizing content
- **SEO professionals** ‚Äî identify redirect chains that dilute link equity across client sites
- **Developers** ‚Äî add `linkcheck` to your CI/CD pipeline to catch broken links before deploy
- **Agencies** ‚Äî generate interactive HTML reports to share with clients

---

## Requirements

- Python 3.10+
- A `sitemap.xml` on your target site

### Dependencies

- `requests` ‚Äî HTTP client
- `beautifulsoup4` + `lxml` ‚Äî HTML parsing
- `pandas` ‚Äî report generation
- `tqdm` ‚Äî progress bars
- `urllib3` ‚Äî URL handling

All dependencies install automatically via `pip install -e .`

---

## Contributing

Contributions are welcome! Whether it's a bug fix, new feature, or documentation improvement ‚Äî open an issue or submit a pull request.

---

## License

MIT ‚Äî use it however you want, commercially or otherwise.

---

<p align="center">
  <strong>Migrated your site recently? Don't wait for your traffic to drop.</strong><br>
  Run `linkcheck https://yoursite.com/sitemap.xml` and find out what's broken.
</p>
